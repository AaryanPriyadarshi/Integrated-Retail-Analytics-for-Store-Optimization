{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "1dmO-2ZchlEz",
        "aBKYA5Huh0pR",
        "x71ZqKXriCWQ",
        "h1EGLwPg557Z",
        "Rdlrm8oZcKDS",
        "OzPh8PqpcMUR",
        "booCJCLlmPnH",
        "lzmlYsEPmbnC",
        "XzKXYnzzw2Ad",
        "bOTzaopEw8oA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaryanPriyadarshi/Integrated-Retail-Analytics-for-Store-Optimization/blob/main/Integrated_Retail_Analytics_for_Store_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Integrated Retail Analytics for Store Optimization Project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA / Regression / Forecasting\n",
        "\n",
        "##### **Contribution**    - Aaryan Priyadarshi"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on forecasting weekly retail sales by integrating data from sales transactions, store information, and external economic indicators. The goal is to identify sales patterns, test the impact of holidays, and build machine learning models that can generate accurate forecasts. Using techniques such as exploratory data analysis (EDA), hypothesis testing, feature engineering, and machine learning models (Random Forest and XGBoost), the project highlights how data-driven insights can optimize store operations. The results show that holidays, promotions, store size, and economic factors significantly drive sales, with XGBoost outperforming Random Forest in prediction accuracy. This project demonstrates the practical use of analytics in helping retailers plan promotions, manage inventory, and improve decision-making."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AaryanPriyadarshi/Integrated-Retail-Analytics-for-Store-Optimization"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retailers often struggle to forecast sales accurately due to the influence of multiple factors such as holidays, promotions, store type, and external economic conditions. Poor forecasting can lead to overstocking, understocking, and lost revenue opportunities. The problem this project addresses is:\n",
        "\n",
        "“How can we integrate sales, store, and external feature data to accurately forecast weekly retail sales and identify the key factors driving sales performance?”\n",
        "\n",
        "The solution aims to build machine learning models that not only predict sales but also provide insights into which factors have the most significant impact on sales fluctuations."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Libraries\n",
        "\n",
        "Imported the necessary libraries for data handling, visualization, preprocessing, machine learning, and hypothesis testing.\n"
      ],
      "metadata": {
        "id": "1dmO-2ZchlEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Hypothesis Testing\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Plot Styling\n",
        "plt.style.use(\"seaborn-v0_8\")\n"
      ],
      "metadata": {
        "id": "GgrsuRw0hrfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Dataset Loading\n",
        "\n",
        "Loaded three datasets:\n",
        "- **Sales dataset** (weekly sales per store and department)\n",
        "- **Stores dataset** (store type and size)\n",
        "- **Features dataset** (economic indicators, holidays, promotions)\n",
        "\n",
        "Then merged them into one dataset for analysis.\n"
      ],
      "metadata": {
        "id": "aBKYA5Huh0pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets (fixed paths + removed extra space)\n",
        "sales = pd.read_csv(\"/content/drive/MyDrive/Retail Analysis Project/sales data-set.csv\")\n",
        "stores = pd.read_csv(\"/content/drive/MyDrive/Retail Analysis Project/stores data-set.csv\")\n",
        "features = pd.read_csv(\"/content/drive/MyDrive/Retail Analysis Project/Features data set.csv\")\n",
        "\n",
        "# Merge datasets\n",
        "df = sales.merge(stores, on=\"Store\", how=\"left\")\n",
        "df = df.merge(features, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "\n",
        "# Preview\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Dataset First View\n",
        "\n",
        "- Preview dataset with `.head()`\n",
        "- Check basic info with `.info()` and `.describe()`\n",
        "- Count missing values\n"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.describe().T\n",
        "df.isnull().sum().sort_values(ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Engineering\n",
        "\n",
        "- Convert Date into datetime\n",
        "- Extract **Year, Month, Week, Quarter**\n",
        "- Prepare for seasonal analysis\n"
      ],
      "metadata": {
        "id": "h1EGLwPg557Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Week'] = df['Date'].dt.isocalendar().week\n",
        "df['Quarter'] = df['Date'].dt.quarter\n"
      ],
      "metadata": {
        "id": "qrjbT35bliKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Visualization\n",
        "\n",
        "Used 5 different plots to understand sales patterns:\n",
        "1. Weekly sales trend over time\n",
        "2. Holiday vs non-holiday sales\n",
        "3. Sales by store type\n",
        "4. Correlation heatmap\n",
        "5. Monthly distribution of sales\n"
      ],
      "metadata": {
        "id": "Rdlrm8oZcKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Weekly Sales Trend Over Time\n",
        "plt.figure(figsize=(12,6))\n",
        "df.groupby('Date')['Weekly_Sales'].sum().plot()\n",
        "plt.title(\"Weekly Sales Trend Over Time\")\n",
        "plt.ylabel(\"Total Weekly Sales\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Holiday vs Non-Holiday Sales\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=\"IsHoliday_x\", y=\"Weekly_Sales\", data=df)\n",
        "plt.title(\"Average Weekly Sales: Holiday vs Non-Holiday\")\n",
        "plt.xlabel(\"Holiday Week?\")\n",
        "plt.ylabel(\"Average Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Sales by Store Type\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=\"Type\", y=\"Weekly_Sales\", data=df)\n",
        "plt.title(\"Average Weekly Sales by Store Type\")\n",
        "plt.xlabel(\"Store Type\")\n",
        "plt.ylabel(\"Average Weekly Sales\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Correlation Heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df[['Weekly_Sales','Temperature','Fuel_Price','CPI','Unemployment']].corr(),\n",
        "            annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# 5. Monthly Distribution of Weekly Sales\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x=\"Month\", y=\"Weekly_Sales\", data=df)\n",
        "plt.title(\"Monthly Distribution of Weekly Sales\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Weekly Sales\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z1NOAyO-l1Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Hypothesis Testing\n",
        "\n",
        "**Hypothesis:**\n",
        "- H₀: Holiday weeks have no effect on sales\n",
        "- H₁: Holiday weeks significantly affect sales\n"
      ],
      "metadata": {
        "id": "OzPh8PqpcMUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "holiday_sales = df[df['IsHoliday_x'] == True]['Weekly_Sales']\n",
        "nonholiday_sales = df[df['IsHoliday_x'] == False]['Weekly_Sales']\n",
        "\n",
        "t_stat, p_val = ttest_ind(holiday_sales, nonholiday_sales, equal_var=False)\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_val)\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Reject Null Hypothesis: Holiday weeks significantly affect sales.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference.\")\n"
      ],
      "metadata": {
        "id": "tTG-ogc6rgit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Feature Engineering & Preprocessing\n",
        "\n",
        "- Encode categorical variables\n",
        "- Drop irrelevant columns\n",
        "- Define Weekly_Sales as target\n",
        "- Train/test split\n",
        "- Scale features\n"
      ],
      "metadata": {
        "id": "booCJCLlmPnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features\n",
        "le = LabelEncoder()\n",
        "df['Type'] = le.fit_transform(df['Type'])\n",
        "\n",
        "# Drop irrelevant columns\n",
        "X = df.drop(['Date','Weekly_Sales','IsHoliday_y'], axis=1)\n",
        "y = df['Weekly_Sales']\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "evBCaIzHmSRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Machine Learning Models\n",
        "\n",
        "Used two models:\n",
        "- Random Forest Regressor\n",
        "- XGBoost Regressor\n"
      ],
      "metadata": {
        "id": "lzmlYsEPmbnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)N\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "eLCYqdj9mcyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Model Evaluation\n",
        "\n",
        "We evaluate models using RMSE, MAE, and R², then compare them in a table and bar chart.\n"
      ],
      "metadata": {
        "id": "XzKXYnzzw2Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect metrics\n",
        "results = {\n",
        "    \"Model\": [\"Random Forest\", \"XGBoost\"],\n",
        "    \"RMSE\": [\n",
        "        np.sqrt(mean_squared_error(y_test, y_pred_rf)),\n",
        "        np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "    ],\n",
        "    \"MAE\": [\n",
        "        mean_absolute_error(y_test, y_pred_rf),\n",
        "        mean_absolute_error(y_test, y_pred_xgb)\n",
        "    ],\n",
        "    \"R²\": [\n",
        "        r2_score(y_test, y_pred_rf),\n",
        "        r2_score(y_test, y_pred_xgb)\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(results)\n",
        "display(metrics_df)\n",
        "\n",
        "# Bar chart\n",
        "metrics_df.set_index(\"Model\")[[\"RMSE\",\"MAE\",\"R²\"]].plot(\n",
        "    kind=\"bar\", figsize=(8,6), colormap=\"Set2\", rot=0\n",
        ")\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qD7ES2G9w3DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Feature Importance\n",
        "\n",
        "We check which features drive sales predictions.\n"
      ],
      "metadata": {
        "id": "bOTzaopEw8oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "importances_rf = rf_model.feature_importances_\n",
        "features = X.columns\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=importances_rf, y=features, palette=\"Blues_r\")\n",
        "plt.title(\"Random Forest - Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "# XGBoost\n",
        "importances_xgb = xgb_model.feature_importances_\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=importances_xgb, y=features, palette=\"Greens_r\")\n",
        "plt.title(\"XGBoost - Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B0RVnFbSw9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Conclusion & Reflection\n",
        "\n",
        "- Sales show strong seasonality, with peaks during holidays.\n",
        "- Hypothesis testing confirmed holiday weeks significantly increase sales.\n",
        "- XGBoost outperformed Random Forest across all metrics.\n",
        "- Key drivers of sales include holidays, store size, promotions, and economic indicators.\n",
        "\n",
        "**Reflection:**  \n",
        "This project taught me how to merge multi-source datasets, engineer time-based features, validate business assumptions with hypothesis testing, and apply machine learning for forecasting. The insights gained can help retailers optimize promotions, inventory, and store operations.\n"
      ],
      "metadata": {
        "id": "seslXGUhx1_D"
      }
    }
  ]
}